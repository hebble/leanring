package com.learning.study.third;

/**
 * https://blog.csdn.net/chuanchengdabing/article/details/119727185 Nginx高可用方案
 */
public class NginxLearning {
    /**
     1.什么是nginx？
        Nginx是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP服务器
        Nginx是一款轻量级的Web服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器 目前使用的最多的web服务器或者代理服务器，像淘宝、新浪、网易、迅雷等都在使用

     2.为什么要用Nginx？
        优点：
             跨平台、配置简单
             非阻塞、高并发连接：处理2-3万并发连接数，官方监测能支持5万并发
             内存消耗小：开启10个nginx才占150M内存 成本低廉：开源
             内置的健康检查功能：如果有一个服务器宕机，会做一个健康检查，再发送的请求就不会发送到宕机的服务器了。重新将请求提交到其他的节点上。
             节省宽带：支持GZIP压缩，可以添加浏览器本地缓存
             稳定性高：宕机的概率非常小
             master/worker结构：一个master进程，生成一个或者多个worker进程
             接收用户请求是异步的：浏览器将请求发送到nginx服务器，它先将用户请求全部接收下来，再一次性发送给后端web服务器，极大减轻了web服务器的压力
             一边接收web服务器的返回数据，一边发送给浏览器客户端
             网络依赖性比较低，只要ping通就可以负载均衡
             可以有多台nginx服务器
             事件驱动：通信机制采用epoll模型

     3.为什么Nginx性能这么高？
        得益于它的事件处理机制： 异步非阻塞事件处理机制：运用了epoll模型，提供了一个队列，排队解决

     4.nginx是如何实现高并发的？
        一个主进程，多个工作进程，每个工作进程可以处理多个请求，每进来一个request，会有一个worker进程去处理。但不是全程的处理，处理到可能发生阻塞的地方，比如向上游
        （后端）服务器转发request，并等待请求返回。那么，这个处理的worker继续处理其他请求，而一旦上游服务器返回了，就会触发这个事件，worker才会来接手，这个request
        才会接着往下走。由于web server的工作性质决定了每个request的大部份生命都是在网络传输中，实际上花费在server机器上的时间片不多。这是几个进程就解决高并发的秘密
        所在。即@skoo所说的webserver刚好属于网络io密集型应用，不算是计算密集型。

     5.为什么不使用多线程？
        因为线程创建和上下文的切换非常消耗资源，线程占用内存大，上下文切换占用cpu也很高，采用epoll模型避免了这个缺点

     6.Nginx是如何处理一个请求的呢？
         首先，nginx在启动时，会解析配置文件，得到需要监听的端口与ip地址，然后在nginx的master进程里面
         先初始化好这个监控的socket(创建socket，设置addrreuse等选项，绑定到指定的ip地址端口，再listen)
         然后再fork(一个现有进程可以调用fork函数创建一个新进程。由fork创建的新进程被称为子进程 )出多个子进程出来
         然后子进程会竞争accept新的连接。此时，客户端就可以向nginx发起连接了。当客户端与nginx进行三次握手，与nginx建立好一个连接后
         此时，某一个子进程会accept成功，得到这个建立好的连接的socket，然后创建nginx对连接的封装，即ngx_connection_t结构体
         接着，设置读写事件处理函数并添加读写事件来与客户端进行数据的交换。最后，nginx或客户端来主动关掉连接，到此，一个连接就寿终正寝了

     7.正向代理和负向代理
        7.1 正向代理
             一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)
             然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理
             正向代理总结就一句话：代理端代理的是客户端
        7.2 负向代理
             反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求，发给内部网络上的服务器
             并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器
             反向代理总结就一句话：代理端代理的是服务端

     8.什么是动态资源、静态资源分离？
         动态资源、静态资源分离是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后
         我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路
         动态资源、静态资源分离简单的概括是：动态文件与静态文件的分离

     9.为什么要做动、静分离？
         在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js等等文件）
         这些不需要经过后台处理的文件称为静态文件，否则动态文件。因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静态文件不就完了吗
         当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决
         动、静分离将网站静态资源（HTML，JavaScript，CSS，img等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问
         这里我们将静态资源放到nginx中，动态资源转发到tomcat服务器中

     10.负载均衡
         负载均衡即是代理服务器将接收的请求均衡的分发到各服务器中
         负载均衡主要解决网络拥塞问题，提高服务器响应速度，服务就近提供，达到更好的访问质量，减少后台服务器大并发压力

     10.Nginx三大功能
         反向代理
         负载均衡
         动静分离

     11.Nginx惊群
         惊群效应（thundering herd）是指多进程（多线程）在同时阻塞等待同一个事件的时候（休眠状态），如果等待的这个事件发生，那么他就会唤醒等待的所有进程（或者线程），
        但是最终却只能有一个进程（线程）获得这个时间的 “控制权”，对该事件进行处理，而其他进程（线程）获取 “控制权” 失败，只能重新进入休眠状态，这种现象和性能浪费就叫做惊群效应。

     12.Nginx开启gzip压缩
         Nginx 开启 Gzip 压缩功能， 可以使网站的 css、js 、xml、html 文件在传输时进行压缩，提高访问速度, 进而优化 Nginx 性能。
         网站加载的速度取决于浏览器必须下载的所有文件的大小。减少要传输的文件的大小可以使网站不仅加载更快，而且对于那些宽带是按量计费的人来说也更友好。
         gzip 是一种流行的数据压缩程序。您可以使用 gzip 压缩 Nginx 实时文件。这些文件在检索时由支持它的浏览器解压缩，好处是 web 服务器和浏览器之间传输的数据量更小，速度更快。
         gzip 不一定适用于所有文件的压缩。例如，文本文件压缩得非常好，通常会缩小两倍以上。另一方面，诸如 JPEG或 PNG 文件之类的图像已经按其性质进行压缩，使用 gzip 压缩很难有好
        的压缩效果或者甚至没有效果。压缩文件会占用服务器资源，因此最好只压缩那些压缩效果好的文件。

     13.Nginx 502错误原因和解决方法
        不管你是做运维还是做开发，哪怕你是游客，时不时会遇到 502 Bad Gateway 或 504 Gateway Time-out。出现这页面，把服务重启下，再实在不行重启下服务器，问题就解决了，特殊情况请继续阅读。

     14.Nginx负载均衡的策略有哪些?
        (1)轮询(默认)
            每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某个服务器宕机，能自动剔除故障系统。
            在http{}模块里添加以下内容
                 upstream webServer {
                     server 192.168.43.131:80;
                     server 192.168.43.132:80;
                 }
                 server{
                     listen 80;
                     server_name 192.168.43.130;
                     location / {
                         index  index.html index.htm;
                         proxy_pass http://webServer;
                     }
                 }
        (2)权重
            weight的值越大，分配到的访问概率越高，主要用于后端每台服务器性能不均衡的情况下。其次是为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源。
             upstream backserver {
                 server 192.168.0.12 weight=2;
                 server 192.168.0.13 weight=8;
             }
        (3)ip_hash( IP绑定)
            每个请求按访问IP的哈希结果分配，使来自同一个IP的访客固定访问一台后端服务器，并且可以有效解决动态网页存在的session共享问题
             upstream backserver {
                 ip_hash;
                 server 192.168.0.12:88;
                  server 192.168.0.13:80;
             }
        (4)fair(第三方插件)
            必须安装upstream_fair模块。
            对比 weight、ip_hash更加智能的负载均衡算法，fair算法可以根据页面大小和加载时间长短智能地进行负载均衡，响应时间短的优先分配。
        (5)url_hash(第三方插件)
            必须安装Nginx的hash软件包
            按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，可以进一步提高后端缓存服务器的效率。

     15.location的作用是什么？
        location指令的作用是根据用户请求的URI来执行不同的应用，也就是根据用户请求的网站URL进行匹配，匹配成功即进行相关的操作。
            location语法
                 匹配符 匹配规则 优先级
                 =    精确匹配    1
                 ^~    以某个字符串开头    2
                 ~    区分大小写的正则匹配    3
                 ~*    不区分大小写的正则匹配    4
                 !~    区分大小写不匹配的正则    5
                 !~*    不区分大小写不匹配的正则    6
                 /    通用匹配，任何请求都会匹配到    7
             Location正则案例
                 #优先级1,精确匹配，根路径
                 location=/{
                    return400;
                 }
                 #优先级2,以某个字符串开头,以av开头的，优先匹配这里，区分大小写
                 location^~/av{
                    root/data/av/;
                 }
                 #优先级3，区分大小写的正则匹配，匹配/media*****路径
                 location~/media{
                    alias/data/static/;
                 }
                 #优先级4，不区分大小写的正则匹配，所有的****.jpg|gif|png都走这里
                 location~*.*\.(jpg|gif|png|js|css)${
                    root/data/av/;
                 }
                 #优先7，通用匹配
                 location/{
                    return403;
                 }

     16.在Nginx中，解释如何在URL中保留双斜线?
         要在URL中保留双斜线，就必须使用merge_slashes_off;
         语法:merge_slashes [on/off]
         默认值: merge_slashes on
         环境: http，server

     17.请解释什么是C10K问题?
        C10K问题是指无法同时处理大量客户端(10,000)的网络套接字。

     18.什么是CDN？CDN和DNS有哪些关系和区别？ https://baijiahao.baidu.com/s?id=1721098433786504052&wfr=spider&for=pc
        18.1 什么是CDN？
            CDN的全称是Content Delivery Network，翻译成中文就是内容分发网络。CDN依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户就近获取内容，降低网络延迟，提高访问速度。简单来讲，CDN是用来进行加速的，它可以让用户更快获得所需的数据。
            举个例子，某个网站的服务器在北京，如果深圳的用户想要获取服务器上的数据，就需要跨越很远的距离，由于传输速度和路由转发等因素，就会导致访问速度非常缓慢。但如果我们在深圳建立一个CDN服务器，上面缓存了一些服务器数据，那么深圳用户只需要访问这个CND的服务器就能获取相关的内容，这样速度就提升了很多。
        18.2 什么是DNS？
             要了解cdn就先要了解一下dns。当我们在浏览器中输入一个域名时，就能访问对应的站点。但实际的情况远比我们看到的复杂，因为计算机不能直接识别域名，所以必须依靠某种环节将域名翻译成IP地址才能，这个环节就是DNS。
             当我们向DNS服务器发起解析域名的请求时，DNS服务器首先会查询自己的缓存中有没有该域名，如果缓存中存在该域名，则可以直接返回IP地址。如果缓存中没有，服务器则会以递归的方式层层访问。
             例如，我们要访问www.baidu.com，首先我们会先向全球13个根服务器发起请求，询问com域名的地址，然后再向负责com域名的名称服务器发送请求，找到baidu.com，这样层层递归，最终找到我们需要的IP地址。
        18.3 CDN和DNS的区别
             上面我们提到CDN实际上提供就是一个就近访问的功能，但现在有一个问题是，我们如何直到用户所在的位置并为其分配最佳的CDN节点呢。这就需要用DNS服务进行定位了。
             当我们使用DNS服务时，可以根据用户使用的递归服务器进行定位。但你给我们看到用户使用的是深圳的递归服务器，那么就认为该用户来自深圳，然后调度服务器就让该客户去访问深圳的CDN服务器。
             但这种调度方式可能存在一个问题，就是用户的实际IP与递归服务器并不一致。比如我是北京联通的用户却使用了深圳电信的递归服务器，那么调度服务器为我分配深圳电信的CDN服务器，就会产生错误的调度。
        18.4 HTTP调度
             针对上面的问题，我们还有另一种调度方式——http调度。
             当用户访问服务器时，先分析用户的IP地址，然后服务器给用户返回一个302重定向，将离用户最近的服务器存在缓存中，用户再去请求时就能得到最佳的CDN节点。
             这种方式定位更加准确，但缺点是需要增加一次额外的HTTP访问，这样导致首次访问的延时比较高。
             所以在实际情况中，我们可以将两种方式结合起来，先通过dns的方式来定位，然后通过http的方式来纠正偏差。
        18.5 缓存的两种方式
             CDN节点中缓存了服务器上的部分资源。那么服务器怎么去更新CDN节点的缓存呢？
             一种方式，是服务器主动进行缓存的更新，CDN节点被动接受。另一种方式是用户请求的资源不存在时，CDN节点主动放弃请求，更新缓存，然后将数据返给用户。
             显然第一种方式存在很多问题，例如很容易产生404等，所以一般采用第二种缓存方式。
        18.6 CDN工作流程
             当用户请求一个文件时，CDN的工作过程如下：
                 1.DNS请求当地local DNS
                 2.当地local DNS递归地查询服务器的gslb
                 3.服务器根据local DNS 分配最佳节点，返回IP
                 4.用户获得最佳接入IP，访问最佳节点。
                 5.如果该节点没有用户想要获取的内容，则通过内部路由访问上一节点，直到找到文件或到达源站为止。
                 6.CDN节点缓存该数据，下次请求该文件时可以直接返回。

     19.Nginx高可用方案keepalived https://blog.csdn.net/chuanchengdabing/article/details/119727185
         通过nginx代理服务器，可以对后端的具体应用实现反向代理或负载均衡等功能，并且nginx可以对应用进行健康检查，将故障节点从负载均衡池中排出，从而实现对后端应用的高可用性保障。
         但是，如果nginx服务器出现问题，则无法对外提供服务, 因此，我们要考虑到nginx服务的高可用性，采用主-备，或主-主的形式安装部署nginx服务，nginx的主备高可用性，依靠keepalived组件实现：
        关键词:
             KeepAlived（主服务器 和 备份服务器 故障时 IP 瞬间无缝交接）
             VRRP协议（路由器组，提供虚拟IP，一个master和多个backup，组播消息，选举backup当master）
             Nginx+keepalived 双机主主模式（俩公网虚拟IP，负载）；双机主从模式（热备服务器）
        19.1 Nginx & KeepAlived
             Nginx 进程基于Master+Slave(worker)多进程模型，自身具有非常稳定的子进程管理功能。在Master进程分配模式下，Master进程永远不进行业务处理，只是进行任务分发，从而达到Master进程的存活高可靠性，
                Slave(worker)进程所有的业务信号都 由主进程发出，Slave(worker)进程所有的超时任务都会被Master中止，属于非阻塞式任务模型。
             KeepAlived是Linux下面实现VRRP备份路由的高可靠性运行件。基于KeepAlived设计的服务模式能够真正做到主服务器和备份服务器故障时IP瞬间无缝交接。二者结合，可以构架出比较稳定的软件LB（Load Balance）方案。
             KeepAlived的作用是检测服务器的状态，如果有一台web服务器宕机，或工作出现故障，KeepAlived将检测到，并将有故障的服务器从系统中剔除，同时使用其他服务器代替该服务器的工作，当服务器工作正常后KeepAlived
                自动将服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的服务器。
             KeepAlived是一个基于VRRP协议来实现的服务高可用方案，可以利用其来避免IP单点故障，类似的工具还有Heartbeat 、Corosync、Pacemaker。但是它一般不会单独出现，而是与其它负载均衡技术（如LVS、Haproxy、Nginx）一起工作来达到集群的高可用
        19.2 VRRP协议
            VRRP全称 Virtual Router Redundancy Protocol，即”虚拟路由冗余协议“。可以认为它是实现路由器高可用的容错协议，即将N台提供相同功能的路由器组成一个路由器组(Router Group)，这个组里面有一个master和多个backup，但在外界看来就像一台一样，
            构成虚拟路由器，拥有一个虚拟IP（VIP - Virtual IP，也就是路由器所在局域网内其他机器的默认路由），占有这个IP的master实际负责ARP相应和转发IP数据包，组中的其它路由器作为备份的角色处于待命状态。master会发组播消息，当backup在超时时间
            内收不到vrrp包时就认为master宕掉了，这时就需要根据VRRP的优先级来选举一个backup当master，保证路由器的高可用。
        19.3 双机高可用解决方案
            (1)Nginx+keepalived 双机主从模式:
                即前端使用两台服务器，一台主服务器和一台热备服务器，正常情况下，主服务器绑定一个公网虚拟IP，提供负载均衡服务，热备服务器处于空闲状态；当主服务器发生故障时，热备服务器接管主服务器的公网虚拟IP，提供负载均衡服务；但是热备服务器在主机器不出现故障的时候，永远处于浪费状态，对于服务器不多的网站，该方案不经济实惠。
            (2)Nginx+keepalived 双机主主模式:
                即前端使用两台负载均衡服务器，互为主备，且都处于活动状态，同时各自绑定一个公网虚拟IP，提供负载均衡服务；当其中一台发生故障时，另一台接管发生故障服务器的公网虚拟IP（这时由非故障机器一台负担所有的请求）。
        19.4 nginx+keepalived 主从, 双主配置
            详见 http://t.zoukankan.com/dyh004-p-8407025.html
     */
}
